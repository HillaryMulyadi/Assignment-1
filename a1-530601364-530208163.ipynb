{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 Assignment 1: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group number: 74  , SID1: 530601364 , SID2: 530208163"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "breast_cancer_df = pd.read_csv(\"breast-cancer-wisconsin.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/48/bt4d4zlj06j9d38_vzf767bh0000gn/T/ipykernel_15323/971942752.py:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  breast_cancer_df.iloc[:, :-1] = imputer.transform(breast_cancer_df.iloc[:, :-1])\n"
     ]
    }
   ],
   "source": [
    "# Pre-process dataset\n",
    "breast_cancer_df = breast_cancer_df.replace('?', np.nan)\n",
    "\n",
    "# Replacing missing values with mean value of the column\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer = imputer.fit(breast_cancer_df.iloc[:, :-1])\n",
    "breast_cancer_df.iloc[:, :-1] = imputer.transform(breast_cancer_df.iloc[:, :-1])\n",
    "\n",
    "# Normalising the values between [0,1]\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(breast_cancer_df.iloc[:, :-1])\n",
    "breast_cancer_df.iloc[:, :-1] = scaler.transform(breast_cancer_df.iloc[:, :-1])\n",
    "\n",
    "# Changing the class values to 0 and 1 respectively\n",
    "breast_cancer_df = breast_cancer_df.replace('class1', '0')\n",
    "breast_cancer_df = breast_cancer_df.replace('class2', '1')\n",
    "breast_cancer_df[\"class\"] = breast_cancer_df[\"class\"].astype(int)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4444,0.0000,0.0000,0.0000,0.1111,0.0000,0.2222,0.0000,0.0000,0\n",
      "0.4444,0.3333,0.3333,0.4444,0.6667,1.0000,0.2222,0.1111,0.0000,0\n",
      "0.2222,0.0000,0.0000,0.0000,0.1111,0.1111,0.2222,0.0000,0.0000,0\n",
      "0.5556,0.7778,0.7778,0.0000,0.2222,0.3333,0.2222,0.6667,0.0000,0\n",
      "0.3333,0.0000,0.0000,0.2222,0.1111,0.0000,0.2222,0.0000,0.0000,0\n",
      "0.7778,1.0000,1.0000,0.7778,0.6667,1.0000,0.8889,0.6667,0.0000,1\n",
      "0.0000,0.0000,0.0000,0.0000,0.1111,1.0000,0.2222,0.0000,0.0000,0\n",
      "0.1111,0.0000,0.1111,0.0000,0.1111,0.0000,0.2222,0.0000,0.0000,0\n",
      "0.1111,0.0000,0.0000,0.0000,0.1111,0.0000,0.0000,0.0000,0.4444,0\n",
      "0.3333,0.1111,0.0000,0.0000,0.1111,0.0000,0.1111,0.0000,0.0000,0\n"
     ]
    }
   ],
   "source": [
    "# Print first ten rows of pre-processed dataset to 4 decimal places as per assignment spec\n",
    "# A function is provided to assist\n",
    "\n",
    "x = breast_cancer_df.drop('class', axis=1).values\n",
    "y = y = breast_cancer_df['class'].values\n",
    "\n",
    "def print_data(X, y, n_rows=10):\n",
    "    \"\"\"Takes a numpy data array and target and prints the first ten rows.\n",
    "    \n",
    "    Arguments:\n",
    "        X: numpy array of shape (n_examples, n_features)\n",
    "        y: numpy array of shape (n_examples)\n",
    "        n_rows: numpy of rows to print\n",
    "    \"\"\"\n",
    "    for example_num in range(n_rows):\n",
    "        for feature in X[example_num]:\n",
    "            print(\"{:.4f}\".format(feature), end=\",\")\n",
    "\n",
    "        if example_num == len(X)-1:\n",
    "            print(y[example_num],end=\"\")\n",
    "        else:\n",
    "            print(y[example_num])\n",
    "            \n",
    "\n",
    "print_data(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Cross-validation without parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the 10 fold stratified cross-validation\n",
    "cvKFold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "# The stratified folds from cvKFold should be provided to the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def logregClassifier(X, y):\n",
    "    logreg = LogisticRegression(solver='liblinear')\n",
    "    scores = cross_val_score(logreg, X, y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Na√Øve Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def nbClassifier(X, y):\n",
    "    nb = GaussianNB()\n",
    "    scores = cross_val_score(nb, X, y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def dtClassifier(X, y):\n",
    "    tree = DecisionTreeClassifier(criterion='entropy', random_state = 0)\n",
    "    scores = cross_val_score(tree, X, y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensembles: Bagging, Ada Boost and Gradient Boosting\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def bagDTClassifier(X, y, n_estimators, max_samples, max_depth):\n",
    "    bag_clt = BaggingClassifier(DecisionTreeClassifier(criterion='entropy', max_depth=max_depth, random_state=0), n_estimators = n_estimators, max_samples = max_samples, random_state=0)\n",
    "    scores = cross_val_score(bag_clt, X, y, cv=cvKFold)\n",
    "    return scores.mean()\n",
    "\n",
    "def adaDTClassifier(X, y, n_estimators, learning_rate, max_depth):\n",
    "    ada_clt = AdaBoostClassifier(DecisionTreeClassifier(criterion='entropy', max_depth=max_depth), n_estimators = n_estimators, learning_rate = learning_rate, random_state=0)\n",
    "    scores = cross_val_score(ada_clt, X, y, cv=cvKFold)\n",
    "    return scores.mean()\n",
    "\n",
    "def gbClassifier(X, y, n_estimators, learning_rate):\n",
    "    gb_clt = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, random_state=0)\n",
    "    scores = cross_val_score(gb_clt, X, y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogR average cross-validation accuracy: 0.9657\n",
      "NB average cross-validation accuracy: 0.9585\n",
      "DT average cross-validation accuracy: 0.9385\n",
      "Bagging average cross-validation accuracy 0.9571\n",
      "AdaBoost average cross-validation accuracy: 0.9599\n",
      "GB average cross-validation accuracy: 0.9613\n"
     ]
    }
   ],
   "source": [
    "# Parameters for Part 1:\n",
    "\n",
    "#Bagging\n",
    "bag_n_estimators = 60\n",
    "bag_max_samples = 100\n",
    "bag_max_depth = 6\n",
    "\n",
    "#AdaBoost\n",
    "ada_n_estimators = 60\n",
    "ada_learning_rate = 0.5\n",
    "ada_bag_max_depth = 6\n",
    "\n",
    "#GB\n",
    "gb_n_estimators = 60\n",
    "gb_learning_rate = 0.5\n",
    "\n",
    "x = breast_cancer_df.iloc[:, :-1]\n",
    "y = breast_cancer_df.iloc[:, -1]\n",
    "\n",
    "# Print results for each classifier in part 1 to 4 decimal places here:\n",
    "print(\"LogR average cross-validation accuracy: {:.4f}\".format(logregClassifier(x, y)))\n",
    "print(\"NB average cross-validation accuracy: {:.4f}\".format(nbClassifier(x, y)))\n",
    "print(\"DT average cross-validation accuracy: {:.4f}\".format(dtClassifier(x, y)))\n",
    "print(\"Bagging average cross-validation accuracy {:.4f}\".format(bagDTClassifier(x, y, bag_n_estimators, bag_max_samples, bag_max_depth)))\n",
    "print(\"AdaBoost average cross-validation accuracy: {:.4f}\".format(adaDTClassifier(x, y, ada_n_estimators, ada_learning_rate, ada_bag_max_depth)))\n",
    "print(\"GB average cross-validation accuracy: {:.4f}\".format(gbClassifier(x, y, gb_n_estimators, gb_learning_rate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Cross-validation with parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k = [1, 3, 5, 7, 9]\n",
    "p = [1, 2]\n",
    "param_grid_knn = {'n_neighbors': k,\n",
    "              'p': p}\n",
    "\n",
    "def bestKNNClassifier(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=0)\n",
    "\n",
    "    grid_search = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=cvKFold,\n",
    "                          return_train_score=True)\n",
    "\n",
    "\n",
    "    grid_search.fit(X_train, y_train) \n",
    "    \n",
    "    test_set_score = grid_search.score(X_test, y_test)\n",
    "    best_params = grid_search.best_params_\n",
    "    best_cross_validation_score = grid_search.best_score_\n",
    "    return best_params['n_neighbors'], best_params['p'], best_cross_validation_score, test_set_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "# You should use SVC from sklearn.svm with kernel set to 'rbf'\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "C = [0.01, 0.1, 1, 5, 15] \n",
    "gamma = [0.01, 0.1, 1, 10, 50]\n",
    "param_grid_svm = {'C': C, 'gamma': gamma}\n",
    "\n",
    "def bestSVMClassifier(X, y):    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=0)\n",
    "   \n",
    "    grid_search = GridSearchCV(SVC(kernel=\"rbf\"), param_grid_svm, cv=cvKFold,\n",
    "                          return_train_score=True)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    test_set_score = grid_search.score(X_test, y_test)\n",
    "    best_params = grid_search.best_params_\n",
    "    best_cross_validation_score = grid_search.best_score_\n",
    "    \n",
    "    return best_params['C'], best_params['gamma'], best_cross_validation_score, test_set_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# You should use RandomForestClassifier from sklearn.ensemble with information gain and max_features set to ‚Äòsqrt‚Äô.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import *\n",
    "import pandas as pd\n",
    "\n",
    "n_estimators = [10, 30, 60, 100, 150]\n",
    "max_leaf_nodes = [6, 12, 18]\n",
    "param_grid_rfc = {'n_estimators': n_estimators, \n",
    "              'max_leaf_nodes': max_leaf_nodes}\n",
    "\n",
    "def bestRFClassifier(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, stratify=y, random_state=0\n",
    "    )\n",
    "\n",
    "    grid_search_rfc = GridSearchCV(RandomForestClassifier(criterion='entropy', random_state=0), param_grid_rfc, cv=cvKFold,\n",
    "                               return_train_score=True)\n",
    "    \n",
    "    grid_search_rfc.fit(X_train, y_train)\n",
    "    \n",
    "    test_set_score = grid_search_rfc.score(X_test, y_test)\n",
    "    best_params = grid_search_rfc.best_params_\n",
    "    best_cross_validation_score = grid_search_rfc.best_score_\n",
    "\n",
    "#classification report to find macro avg F1 score, weighted avg F1 score\n",
    "    rf = RandomForestClassifier(criterion='entropy', random_state=0)\n",
    "    rf.fit(X_train, y_train)\n",
    "    actual = y_test\n",
    "    predicted = rf.predict(X_test)\n",
    "    predictions = metrics.classification_report(actual, predicted, output_dict=True)\n",
    "    macro_avg_f1 = predictions['macro avg']['f1-score']\n",
    "    weighted_avg_f1 = predictions['weighted avg']['f1-score']\n",
    "    return best_params['n_estimators'], best_params['max_leaf_nodes'], best_cross_validation_score, test_set_score, macro_avg_f1, weighted_avg_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN best k: 3\n",
      "KNN best p: 1\n",
      "KNN cross-validation accuracy: 0.9695\n",
      "KNN test set accuracy: 0.9543\n",
      "\n",
      "SVM best C: 5.0000\n",
      "SVM best gamma: 0.1000\n",
      "SVM cross-validation accuracy: 0.9676\n",
      "SVM test set accuracy: 0.9714\n",
      "\n",
      "RF best n_estimators: 150\n",
      "RF best max_leaf_nodes: 6\n",
      "RF cross-validation accuracy: 0.9675\n",
      "RF test set accuracy: 0.9657\n",
      "RF test set macro average F1: 0.9689\n",
      "RF test set weighted average F1: 0.9717\n"
     ]
    }
   ],
   "source": [
    "# Perform Grid Search with 10-fold stratified cross-validation (GridSearchCV in sklearn). \n",
    "# The stratified folds from cvKFold should be provided to GridSearchV\n",
    "\n",
    "# This should include using train_test_split from sklearn.model_selection with stratification and random_state=0\n",
    "# Print results for each classifier here. All results should be printed to 4 decimal places except for\n",
    "# \"k\", \"p\", n_estimators\" and \"max_leaf_nodes\" which should be printed as integers.\n",
    "\n",
    "x = breast_cancer_df.iloc[:, :-1]\n",
    "y = breast_cancer_df.iloc[:, -1]\n",
    "best_k, best_p, cross_val_score, test_set_acc = bestKNNClassifier(x,y)\n",
    "\n",
    "print(\"KNN best k: {}\".format(best_k))\n",
    "print(\"KNN best p: {}\".format(best_p))\n",
    "print(\"KNN cross-validation accuracy: {:.4f}\".format(cross_val_score))\n",
    "print(\"KNN test set accuracy: {:.4f}\".format(test_set_acc))\n",
    "\n",
    "print()\n",
    "\n",
    "best_c_parm, best_gam_parm, cross_val_accuracy, test_set_accuracy = bestSVMClassifier(x, y)\n",
    "print(\"SVM best C: {:.4f}\".format(best_c_parm))\n",
    "print(\"SVM best gamma: {:.4f}\".format(best_gam_parm))\n",
    "print(\"SVM cross-validation accuracy: {:.4f}\".format(cross_val_accuracy))\n",
    "print(\"SVM test set accuracy: {:.4f}\".format(test_set_accuracy))\n",
    "\n",
    "print()\n",
    "\n",
    "best_n_est, max_leaf_nodes, cross_val_acc, test_set_acc, test_set_macro_f1, test_set_macro_avg = bestRFClassifier(x,y)\n",
    "print(\"RF best n_estimators: {}\".format(best_n_est))\n",
    "print(\"RF best max_leaf_nodes: {}\".format(max_leaf_nodes))\n",
    "print(\"RF cross-validation accuracy: {:.4f}\".format(cross_val_acc))\n",
    "print(\"RF test set accuracy: {:.4f}\".format(test_set_acc))\n",
    "print(\"RF test set macro average F1: {:.4f}\".format(test_set_macro_f1))\n",
    "print(\"RF test set weighted average F1: {:.4f}\".format(test_set_macro_avg))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
